# -*- coding: utf-8 -*-
"""Movement_Recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cljDUqYQ8PXjJhL3LxiL1qE1fTH1wRfu

# Movement Recognition System

# Library import:
Here we are going to use pandas(https://pandas.pydata.org/docs/user_guide/index.html), numpy(https://numpy.org/devdocs/user/whatisnumpy.html) and matplotlib(https://matplotlib.org/stable/contents.html).
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.ensemble import RandomForestClassifier
import pickle
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
import os
import pandas as pd
import numpy as np

"""# Read Data:
First, we have to load the data in the data frame.
"""

# df_X = pd.DataFrame()
# df_Y = pd.DataFrame()

# for filename in os.listdir('Dataset/X'):
#     if filename.endswith("csv"):
#         # Your code comes here such as
#         #print(filename)
#         df_X = df_X.append(pd.read_csv('Dataset/X/'+filename))
#         df_Y = df_Y.append(pd.read_csv('Dataset/Y/'+filename))
# #print(df_X.head())
# #print(df_Y.head())

df_relax = pd.read_csv('Dataset/generated_relax.csv')
df_walk = pd.read_csv('Dataset/generated_walk.csv')
df_walk['Label'] = 2

df_relax.info()

df_walk.info()

"""Now let's check what information the data contains!

So, here we can see in the data file there are many rows and columns. Do you want to know the exact number of rows and columns?
"""

#print("df_X shape: ", df_X.shape)
#print("df_Y shape: ", df_Y.shape)

df = df_relax.append(df_walk)
#df['label'] = df_Y['label']

#del df_X
#del df_Y

df.head()

df[df['Label'] == 1].head(10)
df[df['Label'] == 2].head(10)

# df_new = df[df['label'] == 1].copy()
# df_new = df_new.append(df[df['label'] == 15])

# df_new.info()

"""# Data Visualization:

Now, let's see how the data looks like!
"""

# df_Y[df_Y['label']!=0].value_counts().plot.bar()

# df_plot = pd.DataFrame()
# df_plot['Accelerometer X'] = df_new['Accelerometer X']
# df_plot['Accelerometer Y'] = df_new['Accelerometer Y']
# df_plot['Accelerometer Z'] = df_new['Accelerometer Z']
# df_plot['label'] = df_new['label']
# df_plot[df_plot['label'] == 1].plot()

# df_plot[df_plot['label'] == 15].plot()

"""# Pre-processing:
In the preprocessing stage we need to first focus on the missing values. Let's check if our data have any missing values.
"""

# df_new.isnull().sum().sum()

# print(df_new.isnull().sum())

"""We have some missing values. So, we have to keep that in mind while handling the data. To work with this data we will devide the whole data into smaller segments."""

df = df.sort_values(by=["Timestamp UTC"])

df = df.reset_index(drop=False)

df


def segmentation(x_data, overlap_rate, time_window):

    # make a list for segment window and its label
    seg_data = []
    y_segmented_list = []

    # convert overlap rate to step for sliding window
    overlap = int((1 - overlap_rate)*time_window)

    # segment and keep the labels
    for i in range(0, x_data.shape[0], overlap):
        seg_data.append(x_data[i:i+time_window])
        y_segmented_list.append(x_data['Label'][i])

    return seg_data, y_segmented_list


# Segmentation with overlaprate=0 & window=100
# df1_itpl=df.interpolate()
# replace missing values with 0
# df1_itpl=df1_itpl.fillna(0)
# df1_itpl = df1_itpl.reset_index()
[seg, seg_label] = segmentation(df, 0, 8)

"""# Feature Extarction:
There are many types of features. For ease of use we have shown only some very common features.
"""


def get_features(x_data):
    # Set features list
    features = []
    # Set columns name list
    DFclist = list(x_data.columns)

    # Calculate features (STD, Average, Max, Min) for each data columns X Y Z
    for k in DFclist:
        #x_data[k] = [((a-min(x_data[k]))/(max(x_data[k])-min(x_data[k]))) for a in x_data[k]]
        # std
        features.append(x_data[k].std(ddof=0))
        # avg
        features.append(np.average(x_data[k]))
        # max
        features.append(np.max(x_data[k]))
        # min
        features.append(np.min(x_data[k]))
        # median
        features.append(x_data[k].median())
        # mode
        features.append(x_data[k].mode()[0])
    return features


# set list
features_list = []
label_list = []
print(seg[0].columns)
for j in range(0, len(seg)):
    # extract only xyz columns
    frame1 = seg[j].drop(columns=['index', 'Timestamp UTC', 'Label'])

    # Get features and label for each elements
    features_list.append(get_features(frame1))
    label_list.append(seg_label[j])

print(len(features_list[0]))

"""Now we have a feature list and lablel list. Next step is classification.

# Training:

For classification there are several models. Here we are using one of the most commonly used model Random Forest.
"""

model_ml = RandomForestClassifier(n_estimators=500, n_jobs=-1)

"""Here we only have one subject. So, we will divide data from this subject into train and test file to evaluate the results. For more than one subject you can also put one subject in testing and others in training."""

X_train, X_test, y_train, y_test = train_test_split(
    features_list, label_list, test_size=0.3, random_state=42)

"""Now let's train the model!"""

model_ml.fit(X_train, y_train)

"""The training is complete but how can we see the results? For that we will here use classification report with which we can see the accuracy, precision, recall and f1 score. We will also use confusion matrix for the evaluation."""


y_predict = model_ml.predict(X_test)
print(classification_report(y_test, y_predict))

"""We have successfully completed learning to read the data, visualize data, pre-processing, feature extraction, classification and evaluation of the generated model. Now it's your turn to generate a model following these steps and predict the labels of the test data. Best of luck!"""
pickle.dump(model_ml, open('model_relax_vs_walking_fast', 'wb'))
